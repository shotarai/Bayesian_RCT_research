# Complete LLM Prior Elicitation System - Modularized Version
# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«åŒ–ã•ã‚ŒãŸLLMäº‹å‰åˆ†å¸ƒè¨­å®šã‚·ã‚¹ãƒ†ãƒ 

import os
import logging
from datetime import datetime
from typing import Optional
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()
from dotenv import load_dotenv

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ­ãƒ¼ã‚«ãƒ«ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from src import (
    comparative_analysis_setup,
    compare_prior_specifications,
    calculate_sample_size_benefits,
    load_actual_toenail_data,
    save_analysis_results,
    save_summary_report,
    save_prior_comparison_csv,
    create_output_directory_structure
)

# ãƒ­ã‚®ãƒ³ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def run_complete_analysis(api_key: Optional[str] = None, save_results: bool = True):
    """
    å®Œå…¨ãªæ¯”è¼ƒåˆ†æã‚’å®Ÿè¡Œï¼ˆãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›æ©Ÿèƒ½ä»˜ãï¼‰
    
    Parameters:
    -----------
    api_key : Optional[str]
        OpenAI APIã‚­ãƒ¼ (Noneã®å ´åˆã¯Mock LLMä½¿ç”¨)
    save_results : bool
        çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜ã™ã‚‹ã‹ã©ã†ã‹
        
    Returns:
    --------
    dict
        åˆ†æçµæœè¾æ›¸
    """
    print("="*80)
    print("COMPLETE BAYESIAN RCT PRIOR ANALYSIS")
    print("LLM vs Historical vs Uninformative Priors")
    print("="*80)
    
    # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    toenail_data = load_actual_toenail_data()
    
    # æ¯”è¼ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    comparison_setup, llm_priors = comparative_analysis_setup(api_key)
    
    # äº‹å‰åˆ†å¸ƒæ¯”è¼ƒ
    print("\nğŸ“Š PRIOR DISTRIBUTIONS COMPARISON:")
    print("-" * 60)
    for prior_type, priors in comparison_setup.items():
        print(f"\n{prior_type.upper()}:")
        for param, spec in priors.items():
            if isinstance(spec, dict) and 'mu' in spec and 'sigma' in spec:
                print(f"  {param}: Î¼={spec['mu']:.3f}, Ïƒ={spec['sigma']:.3f}")
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºå‰Šæ¸›åŠ¹æœ
    sample_size_benefits = calculate_sample_size_benefits(comparison_setup)
    
    print("\nğŸ’¡ SAMPLE SIZE REDUCTION ANALYSIS:")
    print("-" * 60)
    for benefit in sample_size_benefits:
        print(f"\n{benefit.prior_type.upper()}:")
        print(f"  Effective sample size gain: {benefit.effective_sample_size:.2f}x")
        print(f"  Sample size reduction: {benefit.sample_size_reduction:.1%}")
        print(f"  Potential patients saved: {benefit.patient_savings}")
        print(f"  Improved power: {benefit.power:.3f}")
    
    # LLM vs æ­´å²çš„äº‹å‰åˆ†å¸ƒã®æ¯”è¼ƒ
    if 'historical_fixed' in comparison_setup and 'llm_expert' in comparison_setup:
        llm_vs_historical = compare_prior_specifications(
            comparison_setup['llm_expert'],
            comparison_setup['historical_fixed']
        )
        
        print("\nğŸ” LLM vs HISTORICAL PRIORS COMPARISON:")
        print("-" * 60)
        for comp in llm_vs_historical:
            print(f"\n{comp.parameter}:")
            print(f"  Mean difference: {comp.difference_mean:.3f}")
            print(f"  Std difference: {comp.difference_std:.3f}")
            print(f"  Distribution overlap: {comp.overlap_coefficient:.3f}")
    
    print("\nâœ… Complete analysis finished!")
    print("\nSUMMARY:")
    print("1. LLM-elicited priors provide informative alternatives")
    print("2. Sample size reductions possible with informed priors")
    print("3. Patient savings achievable through better prior knowledge")
    
    # çµæœãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®ä½œæˆ
    results = {
        'comparison_setup': comparison_setup,
        'sample_size_benefits': [
            {
                'prior_type': benefit.prior_type if hasattr(benefit, 'prior_type') else str(benefit),
                'patient_savings': benefit.patient_savings if hasattr(benefit, 'patient_savings') else getattr(benefit, 'patients_saved', 'N/A'),
                'sample_size_reduction': benefit.sample_size_reduction if hasattr(benefit, 'sample_size_reduction') else 'N/A',
                'effective_sample_size_gain': benefit.effective_sample_size_gain if hasattr(benefit, 'effective_sample_size_gain') else 'N/A'
            } for benefit in sample_size_benefits
        ],
        'toenail_data': toenail_data.to_dict() if not toenail_data.empty else {},
        'llm_priors': [
            {
                'parameter': prior.parameter if hasattr(prior, 'parameter') else 'unknown',
                'distribution': prior.distribution if hasattr(prior, 'distribution') else 'unknown',
                'mean': prior.mean if hasattr(prior, 'mean') else 'N/A',
                'std': prior.std if hasattr(prior, 'std') else 'N/A',
                'confidence': prior.confidence if hasattr(prior, 'confidence') else 'N/A',
                'rationale': prior.rationale if hasattr(prior, 'rationale') else 'N/A'
            } for prior in llm_priors
        ],
        'data_stats': {
            'total_observations': len(toenail_data) if not toenail_data.empty else 0,
            'unique_patients': toenail_data['id'].nunique() if not toenail_data.empty and 'id' in toenail_data.columns else 0,
            'treatment_groups': toenail_data['treat'].value_counts().to_dict() if not toenail_data.empty and 'treat' in toenail_data.columns else {}
        },
        'analysis_timestamp': datetime.now().isoformat(),
        'api_key_used': api_key is not None
    }
    
    # ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
    if save_results:
        try:
            print("\nğŸ’¾ Saving analysis results...")
            
            # JSONè©³ç´°çµæœä¿å­˜
            json_path = save_analysis_results(results)
            print(f"ğŸ“„ Detailed results: {json_path}")
            
            # Markdownã‚µãƒãƒªãƒ¼ä¿å­˜
            summary_path = save_summary_report(results)
            print(f"ğŸ“‹ Summary report: {summary_path}")
            
            # äº‹å‰åˆ†å¸ƒæ¯”è¼ƒã®CSVä¿å­˜
            prior_comparisons = compare_prior_specifications(
                comparison_setup.get('llm_expert', {}),
                comparison_setup.get('historical_fixed', {})
            )
            if prior_comparisons:
                csv_path = save_prior_comparison_csv([comp.__dict__ if hasattr(comp, '__dict__') else comp for comp in prior_comparisons])
                print(f"ğŸ“Š Prior comparison CSV: {csv_path}")
            
            print("âœ… All results saved successfully!")
            
        except Exception as e:
            print(f"âš ï¸  Error saving results: {e}")
            print("Analysis completed but files not saved.")
    
    return results


def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°
    """
    # API ã‚­ãƒ¼ã®å–å¾—ï¼ˆç’°å¢ƒå¤‰æ•°ã‹ã‚‰ï¼‰
    api_key = os.getenv('OPENAI_API_KEY')
    
    if not api_key:
        print("â„¹ï¸  OpenAI API key not found - using Mock LLM for demonstration")
        print("To use real LLM, set: export OPENAI_API_KEY='your-api-key'")
        print()
    
    # å®Œå…¨ãªæ¯”è¼ƒåˆ†æå®Ÿè¡Œ
    print("ğŸš€ Starting complete Bayesian RCT prior analysis...")
    results = run_complete_analysis(api_key)
    
    if results:
        print("\nğŸ“ˆ Analysis completed successfully!")
        print(f"âœ“ Comparison setup: {len(results['comparison_setup'])} prior types compared")
        print(f"âœ“ Sample size benefits analyzed for {len(results['sample_size_benefits'])} prior specifications")
        print(f"âœ“ Toenail data: {results['data_stats']['total_observations']} observations")
        print(f"âœ“ LLM priors: {len(results['llm_priors'])} parameters elicited")


if __name__ == "__main__":
    main()
