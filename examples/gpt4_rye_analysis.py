#!/usr/bin/env python3
"""
GPT-4 Posterior Predictive Analysis for Rye Project
Rye„Éó„É≠„Ç∏„Çß„ÇØ„ÉàÁî®„ÅÆGPT-4 Posterior Predictive PerformanceÂàÜÊûê
"""

import os
import sys
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
from pathlib import Path

# Load environment variables from .env
def load_env_file():
    """Load .env file manually for Rye project"""
    env_path = Path(__file__).parent.parent / '.env'
    if env_path.exists():
        with open(env_path) as f:
            for line in f:
                if line.strip() and not line.startswith('#'):
                    key, value = line.strip().split('=', 1)
                    os.environ[key] = value
        print(f"‚úÖ Loaded environment from {env_path}")
    else:
        print(f"‚ö†Ô∏è  .env file not found at {env_path}")

# Load environment
load_env_file()

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src import (
    ProductionLLMPriorElicitor,
    MockLLMPriorElicitor,
    PosteriorPredictiveEvaluator,
    generate_synthetic_test_data,
    load_historical_priors
)

def run_gpt4_posterior_predictive_analysis():
    """
    GPT-4„Çí‰ΩøÁî®„Åó„ÅüPosterior Predictive Performance Analysis
    """
    print("üöÄ GPT-4 Posterior Predictive Performance Analysis")
    print("="*60)
    
    # Get API key
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("‚ùå OPENAI_API_KEY not found in environment variables")
        return None
    
    print(f"‚úÖ API Key found: {api_key[:20]}...{api_key[-10:]}")
    
    # Initialize GPT-4 elicitor
    try:
        print("\nü§ñ Initializing GPT-4 Prior Elicitor...")
        gpt4_elicitor = ProductionLLMPriorElicitor(
            api_key=api_key,
            model="gpt-4",
            temperature=0.2
        )
        print("‚úÖ GPT-4 connection successful!")
        using_real_gpt4 = True
        
    except Exception as e:
        print(f"‚ùå GPT-4 initialization failed: {e}")
        print("üîÑ Using Mock LLM for demonstration...")
        gpt4_elicitor = MockLLMPriorElicitor()
        using_real_gpt4 = False
    
    # 1. Elicit priors from GPT-4
    print(f"\nüìã Eliciting priors from {'GPT-4' if using_real_gpt4 else 'Mock LLM'}...")
    
    gpt4_priors = gpt4_elicitor.elicit_clinical_priors(
        treatment_1="Itraconazole 250mg daily for 12 weeks",
        treatment_2="Terbinafine 250mg daily for 12 weeks", 
        outcome_measure="unaffected nail length in millimeters",
        clinical_context="toenail fungal infection (onychomycosis) in adults",
        additional_context="""
        Previous Study2 research assumptions:
        - Baseline nail length: ~2.5mm (actual data shows 1.89mm)
        - Monthly growth rate: ~0.6mm/month (actual: 0.558mm/month)
        - Treatment difference: unknown (actual: 0.042mm/month)
        - Measurement error: ~3.7mm std (actual: 4.39mm)
        
        Please provide expert clinical priors considering both historical assumptions and potential uncertainty.
        """
    )
    
    print(f"‚úÖ Received {len(gpt4_priors)} prior specifications")
    
    # Display obtained priors
    print(f"\nüìä {'GPT-4' if using_real_gpt4 else 'Mock'} Prior Specifications:")
    print("-" * 60)
    for prior in gpt4_priors:
        print(f"üîπ {prior.parameter}:")
        print(f"   Distribution: {prior.distribution}")
        print(f"   Mean (Œº): {prior.mean:.3f}")
        print(f"   Std (œÉ): {prior.std:.3f}")
        print(f"   Confidence: {prior.confidence:.1%}")
        print(f"   Model: {prior.llm_model}")
        print(f"   Rationale: {prior.rationale}")
        print()
    
    # 2. Convert to analysis format
    gpt4_analysis_priors = gpt4_elicitor.export_priors_for_analysis(gpt4_priors)
    
    # 3. Setup comparison with historical priors
    historical_priors = load_historical_priors()
    
    comparison_setup = {
        'gpt4_expert': gpt4_analysis_priors,
        'historical_fixed': historical_priors['fixed_effect_model'],
        'historical_mixed': historical_priors['mixed_effect_model'],
        'uninformative': {
            'beta_intercept': {'dist': 'normal', 'mu': 0, 'sigma': 100},
            'beta_time': {'dist': 'normal', 'mu': 0, 'sigma': 100},
            'beta_interaction': {'dist': 'normal', 'mu': 0, 'sigma': 100}
        }
    }
    
    # 4. Generate test data
    print("\nüî¨ Generating synthetic test data...")
    test_data = generate_synthetic_test_data(n_test=300)
    print(f"‚úÖ Generated {len(test_data)} test observations")
    
    # 5. Run Posterior Predictive Performance Evaluation
    print("\nüìà Running Posterior Predictive Performance Evaluation...")
    evaluator = PosteriorPredictiveEvaluator(baseline_sample_size=400)
    
    # Test multiple sample sizes
    sample_sizes = [50, 100, 150, 200, 250, 300, 350, 400, 500]
    
    performance_results = evaluator.evaluate_prior_performance(
        comparison_setup,
        test_data,
        sample_sizes=sample_sizes
    )
    
    print(f"‚úÖ Completed {len(performance_results)} performance evaluations")
    
    # 6. Analyze and display results
    print("\nüìä POSTERIOR PREDICTIVE PERFORMANCE RESULTS")
    print("="*80)
    
    # Create results table
    print(f"{'Prior Type':<18} {'œÉ':<8} {'n':<6} {'Log-Like':<12} {'Coverage':<10} {'RMSE':<10} {'Rank':<6}")
    print("-" * 80)
    
    # Sort by performance and show top results
    top_results = sorted(performance_results, key=lambda x: x.predictive_log_likelihood, reverse=True)[:20]
    
    for result in top_results:
        print(f"{result.prior_type:<18} {result.sigma_value:<8.3f} {result.sample_size:<6} "
              f"{result.predictive_log_likelihood:<12.2f} {result.coverage_probability:<10.3f} "
              f"{result.rmse:<10.3f} {result.rank:<6}")
    
    # 7. Analysis at baseline sample size (n=400)
    print(f"\nüéØ Performance Analysis at n=400:")
    print("-" * 50)
    
    n400_results = [r for r in performance_results if r.sample_size == 400]
    n400_results = sorted(n400_results, key=lambda x: x.predictive_log_likelihood, reverse=True)
    
    for i, result in enumerate(n400_results):
        rank_emoji = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else f"{i+1}."
        print(f"{rank_emoji} {result.prior_type}")
        print(f"     œÉ: {result.sigma_value:.3f}")
        print(f"     Log-Likelihood: {result.predictive_log_likelihood:.3f}")
        print(f"     Coverage: {result.coverage_probability:.1%}")
        print(f"     RMSE: {result.rmse:.3f}")
        print()
    
    # 8. Sigma optimization for GPT-4 treatment effect
    print("\nüîç Optimizing GPT-4 Treatment Effect Prior Width...")
    
    # Get current sigma from GPT-4
    gpt4_treatment_sigma = None
    for prior in gpt4_priors:
        if prior.parameter in ['treatment_advantage', 'beta_interaction']:
            gpt4_treatment_sigma = prior.std
            break
    
    if gpt4_treatment_sigma:
        print(f"üìã Current GPT-4 treatment effect œÉ: {gpt4_treatment_sigma:.3f}")
        
        # Run sigma optimization
        sigma_values = np.logspace(-2.5, 1.5, 20)  # 0.003 to ~31.6
        
        sigma_benchmark = evaluator.benchmark_sigma_values(
            comparison_setup,
            test_data,
            parameter='beta_interaction',
            sigma_values=sigma_values
        )
        
        print(f"\nüèÜ Sigma Optimization Results:")
        print(f"Best performing œÉ: {sigma_benchmark.best_performing_prior}")
        print(f"Top 5 œÉ values: {', '.join(sigma_benchmark.performance_ranking[:5])}")
        
        # Extract optimal sigma value safely
        optimal_sigma_str = sigma_benchmark.best_performing_prior
        if optimal_sigma_str and '=' in optimal_sigma_str:
            optimal_sigma = float(optimal_sigma_str.split('=')[1])
            improvement_factor = optimal_sigma / gpt4_treatment_sigma
        else:
            # Handle case where format is different or None
            optimal_sigma = None
            improvement_factor = None
        
        
        print(f"\nüí° GPT-4 Prior Width Analysis:")
        print(f"   Current GPT-4 œÉ: {gpt4_treatment_sigma:.3f}")
        
        if optimal_sigma is not None:
            print(f"   Optimal œÉ: {optimal_sigma:.3f}")
            print(f"   Ratio (optimal/current): {improvement_factor:.1f}x")
            
            if improvement_factor > 3:
                recommendation = "‚ö†Ô∏è  GPT-4 prior is too narrow! Consider increasing œÉ by 3x+"
            elif improvement_factor > 1.5:
                recommendation = "‚ö†Ô∏è  GPT-4 prior may be too narrow. Consider increasing œÉ"
            elif improvement_factor < 0.5:
                recommendation = "‚ö†Ô∏è  GPT-4 prior may be too wide. Consider decreasing œÉ"
            else:
                recommendation = "‚úÖ GPT-4 prior width appears reasonable"
        else:
            recommendation = "‚ö†Ô∏è  Could not determine optimal sigma from benchmark results"
            recommendation = "‚úÖ GPT-4 prior width is reasonably calibrated"
        
        print(f"   Recommendation: {recommendation}")
    
    # 9. Generate visualizations
    print(f"\nüìä Generating visualizations...")
    
    # Create results directory if it doesn't exist
    results_dir = project_root / 'results'
    results_dir.mkdir(exist_ok=True)
    
    # Plot 1: Performance vs Sample Size
    plt.figure(figsize=(14, 8))
    
    colors = {
        'gpt4_expert': 'red', 
        'historical_fixed': 'blue', 
        'historical_mixed': 'green',
        'uninformative': 'gray'
    }
    markers = {
        'gpt4_expert': 'o', 
        'historical_fixed': 's', 
        'historical_mixed': '^',
        'uninformative': 'x'
    }
    
    # Group results by prior type
    results_by_prior = {}
    for result in performance_results:
        if result.prior_type not in results_by_prior:
            results_by_prior[result.prior_type] = []
        results_by_prior[result.prior_type].append(result)
    
    for prior_type, type_results in results_by_prior.items():
        if prior_type == 'uninformative':
            continue
            
        sample_sizes = [r.sample_size for r in type_results]
        performances = [r.predictive_log_likelihood for r in type_results]
        
        color = colors.get(prior_type, 'orange')
        marker = markers.get(prior_type, 'o')
        
        plt.plot(sample_sizes, performances, 
                marker=marker, linestyle='-', linewidth=2.5, markersize=8,
                label=f'{prior_type.replace("_", " ").title()}', 
                color=color, alpha=0.8)
    
    plt.xlabel('Sample Size (n)', fontsize=14)
    plt.ylabel('Posterior Predictive Log-Likelihood', fontsize=14)
    plt.title(f'{"GPT-4" if using_real_gpt4 else "Mock"} vs Historical Priors: Posterior Predictive Performance\n(Higher is Better)', 
              fontsize=16, fontweight='bold')
    plt.legend(fontsize=12)
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    # Save performance plot
    perf_plot_path = results_dir / f'gpt4_posterior_predictive_performance_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
    plt.savefig(perf_plot_path, dpi=300, bbox_inches='tight')
    print(f"‚úÖ Performance plot saved: {perf_plot_path}")
    plt.show()
    
    # Plot 2: Sigma Optimization (if available)
    if 'sigma_benchmark' in locals():
        evaluator.plot_sigma_optimization(
            sigma_benchmark,
            save_path=results_dir / f'gpt4_sigma_optimization_{datetime.now().strftime("%Y%m%d_%H%M%S")}.png'
        )
        print(f"‚úÖ Sigma optimization plot saved")
    
    # 10. Save detailed results
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Prepare comprehensive results
    detailed_results = {
        'analysis_info': {
            'timestamp': timestamp,
            'api_type': 'GPT-4' if using_real_gpt4 else 'Mock',
            'model': gpt4_elicitor.model if hasattr(gpt4_elicitor, 'model') else 'mock',
            'test_data_size': len(test_data),
            'sample_sizes_tested': sample_sizes
        },
        'gpt4_priors': [
            {
                'parameter': p.parameter,
                'distribution': p.distribution,
                'mean': p.mean,
                'std': p.std,
                'confidence': p.confidence,
                'rationale': p.rationale,
                'model': p.llm_model,
                'timestamp': p.timestamp
            } for p in gpt4_priors
        ],
        'performance_results': [
            {
                'prior_type': r.prior_type,
                'sigma_value': r.sigma_value,
                'sample_size': r.sample_size,
                'predictive_log_likelihood': r.predictive_log_likelihood,
                'coverage_probability': r.coverage_probability,
                'prediction_interval_width': r.prediction_interval_width,
                'bias': r.bias,
                'rmse': r.rmse,
                'rank': r.rank
            } for r in performance_results
        ],
        'optimization_results': {
            'current_treatment_sigma': gpt4_treatment_sigma,
            'optimal_sigma': optimal_sigma if 'optimal_sigma' in locals() else None,
            'improvement_factor': improvement_factor if 'improvement_factor' in locals() else None,
            'best_performing_prior': sigma_benchmark.best_performing_prior if 'sigma_benchmark' in locals() else None,
            'top_sigma_values': sigma_benchmark.performance_ranking[:5] if 'sigma_benchmark' in locals() else None
        } if gpt4_treatment_sigma else None
    }
    
    # Save JSON results
    results_file = results_dir / f'gpt4_detailed_results_{timestamp}.json'
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
    
    print(f"‚úÖ Detailed results saved: {results_file}")
    
    # 11. Generate COPYABLE SUMMARY
    print("\n" + "="*80)
    print("üìã COPYABLE RESULTS SUMMARY")
    print("="*80)
    
    copyable_summary = f"""
# GPT-4 Posterior Predictive Performance Analysis Results

**Analysis Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}  
**API Used**: {'Real GPT-4' if using_real_gpt4 else 'Mock LLM'}  
**Test Data Size**: {len(test_data)} observations  
**Sample Sizes Tested**: {', '.join(map(str, sample_sizes))}  

## üìä GPT-4 Prior Specifications

```
{'GPT-4' if using_real_gpt4 else 'Mock'} Prior Parameters:
"""
    
    for prior in gpt4_priors:
        copyable_summary += f"\n{prior.parameter}:\n"
        copyable_summary += f"  Œº = {prior.mean:.3f}, œÉ = {prior.std:.3f}\n"
        copyable_summary += f"  Confidence: {prior.confidence:.1%}\n"
        copyable_summary += f"  Rationale: {prior.rationale}\n"
    
    copyable_summary += "\n```\n\n## üèÜ Performance Results (n=400)\n\n```\n"
    
    for i, result in enumerate(n400_results):
        rank = "ü•á" if i == 0 else "ü•à" if i == 1 else "ü•â" if i == 2 else f"{i+1}."
        copyable_summary += f"{rank} {result.prior_type}\n"
        copyable_summary += f"    œÉ: {result.sigma_value:.3f}\n"
        copyable_summary += f"    Log-Likelihood: {result.predictive_log_likelihood:.3f}\n"
        copyable_summary += f"    Coverage: {result.coverage_probability:.1%}\n"
        copyable_summary += f"    RMSE: {result.rmse:.3f}\n\n"
    
    if 'optimal_sigma' in locals():
        copyable_summary += f"```\n\n## üéØ Sigma Optimization Results\n\n```\n"
        copyable_summary += f"Current GPT-4 œÉ (treatment): {gpt4_treatment_sigma:.3f}\n"
        copyable_summary += f"Optimal œÉ: {optimal_sigma:.3f}\n"
        copyable_summary += f"Improvement Factor: {improvement_factor:.1f}x\n"
        copyable_summary += f"Recommendation: {recommendation}\n"
    
    copyable_summary += f"```\n\n## üìà Key Findings\n\n"
    
    gpt4_rank = next((i+1 for i, r in enumerate(n400_results) if r.prior_type == 'gpt4_expert'), 'N/A')
    best_prior = n400_results[0]
    gpt4_result = next((r for r in n400_results if r.prior_type == 'gpt4_expert'), None)
    
    copyable_summary += f"1. **GPT-4 Performance Rank**: {gpt4_rank} out of {len(n400_results)} priors\n"
    copyable_summary += f"2. **Best Performing Prior**: {best_prior.prior_type} (œÉ={best_prior.sigma_value:.3f})\n"
    
    if gpt4_result:
        performance_gap = best_prior.predictive_log_likelihood - gpt4_result.predictive_log_likelihood
        copyable_summary += f"3. **Performance Gap**: {performance_gap:.3f} log-likelihood units\n"
        copyable_summary += f"4. **GPT-4 Coverage**: {gpt4_result.coverage_probability:.1%} (target: 95%)\n"
    
    if 'improvement_factor' in locals():
        if improvement_factor > 2:
            copyable_summary += f"5. **Prior Width Issue**: GPT-4 prior is {improvement_factor:.1f}x too narrow\n"
        elif improvement_factor < 0.5:
            copyable_summary += f"5. **Prior Width Issue**: GPT-4 prior is {1/improvement_factor:.1f}x too wide\n"
        else:
            copyable_summary += f"5. **Prior Width**: GPT-4 prior is reasonably calibrated\n"
    
    copyable_summary += f"\n## üìÅ Generated Files\n\n"
    copyable_summary += f"- Performance plot: `{perf_plot_path.name}`\n"
    if 'sigma_benchmark' in locals():
        copyable_summary += f"- Sigma optimization plot: Generated\n"
    copyable_summary += f"- Detailed results: `{results_file.name}`\n"
    
    print(copyable_summary)
    
    # Save summary as markdown
    summary_file = results_dir / f'gpt4_analysis_summary_{timestamp}.md'
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write(copyable_summary)
    
    print(f"\n‚úÖ Copyable summary saved: {summary_file}")
    print(f"\nüéâ Analysis Complete! Check the results/ directory for outputs.")
    
    return detailed_results

if __name__ == "__main__":
    # Run the analysis
    print("üöÄ Starting GPT-4 Posterior Predictive Performance Analysis...")
    print("üìÅ Project managed with Rye")
    
    results = run_gpt4_posterior_predictive_analysis()
    
    if results:
        print(f"\n‚ú® Analysis completed successfully!")
        print(f"üìä Total evaluations: {len(results['performance_results'])}")
        print(f"ü§ñ Using: {results['analysis_info']['api_type']}")
    else:
        print(f"\n‚ùå Analysis failed to complete")
