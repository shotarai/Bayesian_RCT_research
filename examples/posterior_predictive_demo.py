# filepath: examples/posterior_predictive_demo.py
"""
Posterior Predictive Performance Evaluation Demo
ãƒŸãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã§ææ¡ˆã•ã‚ŒãŸå†…å®¹ã®å®Ÿè£…ä¾‹
"""

import os
import sys
import numpy as np
import matplotlib.pyplot as plt

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src import (
    PosteriorPredictiveEvaluator, 
    generate_synthetic_test_data,
    load_historical_priors
)

def main():
    """
    Posterior Predictive Performance Evaluation ãƒ‡ãƒ¢
    
    å®Ÿè£…å†…å®¹:
    1. äº‹å‰åˆ†å¸ƒã®åˆ†æ•£(Ïƒ)ã‚’å¤‰åŒ–ã•ã›ã¦æ€§èƒ½è©•ä¾¡
    2. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º vs äºˆæ¸¬æ€§èƒ½ã®ãƒ—ãƒ­ãƒƒãƒˆ
    3. æœ€é©ãªÏƒå€¤ã®æ¢ç´¢
    """
    print("ğŸ”¬ Posterior Predictive Performance Evaluation Demo")
    print("="*60)
    
    # 1. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®ç”Ÿæˆ
    print("\nğŸ“Š Generating synthetic test data...")
    test_data = generate_synthetic_test_data(n_test=200)
    print(f"âœ“ Generated {len(test_data)} test observations")
    
    # 2. äº‹å‰åˆ†å¸ƒè¨­å®šã®å–å¾—
    print("\nğŸ“‹ Loading prior configurations...")
    historical_priors = load_historical_priors()
    
    # LLMäº‹å‰åˆ†å¸ƒï¼ˆç¾åœ¨ã®è¨­å®šï¼‰
    llm_priors = {
        'beta_intercept': {'dist': 'normal', 'mu': 2.5, 'sigma': 1.0},
        'beta_time': {'dist': 'normal', 'mu': 0.6, 'sigma': 0.2},
        'beta_interaction': {'dist': 'normal', 'mu': 0.0, 'sigma': 0.15}
    }
    
    comparison_setup = {
        'historical_fixed': historical_priors['fixed_effect_model'],
        'llm_expert': llm_priors,
        'uninformative': {
            'beta_intercept': {'dist': 'normal', 'mu': 0, 'sigma': 100},
            'beta_time': {'dist': 'normal', 'mu': 0, 'sigma': 100},
            'beta_interaction': {'dist': 'normal', 'mu': 0, 'sigma': 100}
        }
    }
    
    # 3. Posterior Predictive Evaluatorã®åˆæœŸåŒ–
    evaluator = PosteriorPredictiveEvaluator(baseline_sample_size=400)
    
    # 4. ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º vs æ€§èƒ½ã®è©•ä¾¡
    print("\nğŸ¯ Evaluating performance across sample sizes...")
    sample_sizes = [50, 100, 150, 200, 250, 300, 350, 400]
    
    performance_results = evaluator.evaluate_prior_performance(
        comparison_setup, 
        test_data, 
        sample_sizes=sample_sizes
    )
    
    print(f"âœ“ Completed {len(performance_results)} evaluations")
    
    # çµæœè¡¨ç¤º
    print("\nğŸ“ˆ Performance Results (Top 10):")
    print("-" * 80)
    print(f"{'Prior Type':<20} {'Ïƒ':<8} {'n':<6} {'Log-Like':<12} {'Coverage':<10} {'RMSE':<8}")
    print("-" * 80)
    
    for result in performance_results[:10]:
        print(f"{result.prior_type:<20} {result.sigma_value:<8.3f} {result.sample_size:<6} "
              f"{result.predictive_log_likelihood:<12.2f} {result.coverage_probability:<10.3f} "
              f"{result.rmse:<8.3f}")
    
    # 5. ãƒ—ãƒ­ãƒƒãƒˆä½œæˆ
    print("\nğŸ“Š Creating performance plots...")
    
    # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º vs æ€§èƒ½ã®ãƒ—ãƒ­ãƒƒãƒˆ
    evaluator.plot_performance_vs_sample_size(
        performance_results,
        save_path="results/posterior_predictive_performance_vs_n.png"
    )
    
    # 6. Ïƒå€¤ã®æœ€é©åŒ–å®Ÿé¨“
    print("\nğŸ” Optimizing sigma values...")
    
    # treatment_advantage (Î²_interaction) ã®Ïƒã‚’å¤‰åŒ–ã•ã›ã¦å®Ÿé¨“
    sigma_values = np.logspace(-2, 1, 15)  # 0.01 to 10
    
    benchmark_result = evaluator.benchmark_sigma_values(
        comparison_setup,
        test_data,
        parameter='beta_interaction',
        sigma_values=sigma_values
    )
    
    print(f"\nğŸ† Best performing sigma values for beta_interaction:")
    print(f"Best: {benchmark_result.best_performing_prior}")
    print(f"Top 5: {', '.join(benchmark_result.performance_ranking)}")
    
    # Ïƒæœ€é©åŒ–ãƒ—ãƒ­ãƒƒãƒˆ
    evaluator.plot_sigma_optimization(
        benchmark_result,
        save_path="results/sigma_optimization_beta_interaction.png"
    )
    
    # 7. è¦ç´„ãƒ¬ãƒãƒ¼ãƒˆ
    print("\nğŸ“‹ Summary Report:")
    print("="*60)
    
    # æœ€é©ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã§ã®æ¯”è¼ƒ
    best_n_results = [r for r in performance_results if r.sample_size == 400]
    best_n_results.sort(key=lambda x: x.predictive_log_likelihood, reverse=True)
    
    print(f"\nAt n=400 (baseline sample size):")
    for i, result in enumerate(best_n_results[:3]):
        print(f"{i+1}. {result.prior_type} (Ïƒ={result.sigma_value:.3f}): "
              f"Log-Likelihood = {result.predictive_log_likelihood:.2f}")
    
    # ç¾åœ¨ã®è¨­å®šã®å•é¡Œç‚¹
    llm_result = next((r for r in best_n_results if r.prior_type == 'llm_expert'), None)
    if llm_result:
        print(f"\nğŸ’¡ Current LLM Prior Analysis:")
        print(f"   - Current Ïƒ for treatment effect: {llm_result.sigma_value:.3f}")
        print(f"   - Performance rank: {llm_result.rank} out of {len(best_n_results)}")
        print(f"   - Coverage probability: {llm_result.coverage_probability:.3f}")
        
        if llm_result.coverage_probability < 0.90:
            print(f"   âš ï¸  Coverage is low - prior might be too narrow!")
        elif llm_result.coverage_probability > 0.98:
            print(f"   âš ï¸  Coverage is too high - prior might be too wide!")
        else:
            print(f"   âœ“ Coverage looks reasonable")
    
    print(f"\nğŸ¯ Recommendations:")
    print(f"   1. Consider testing wider sigma values (current: 0.15-1.0)")
    print(f"   2. Optimal sigma for treatment effect appears to be: {benchmark_result.best_performing_prior}")
    print(f"   3. Use posterior predictive checks to validate prior choices")
    print(f"   4. Consider adaptive prior selection based on data characteristics")
    
    print(f"\nâœ“ Demo completed! Check results/ folder for plots.")

if __name__ == "__main__":
    # çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs("results", exist_ok=True)
    main()
