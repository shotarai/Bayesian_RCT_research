# filepath: src/llm_elicitor.py
"""
LLM Prior Elicitation Classes
LLMãƒ™ãƒ¼ã‚¹ã®äº‹å‰åˆ†å¸ƒè¨­å®šã‚·ã‚¹ãƒ†ãƒ 
"""

import os
import json
import time
import logging
import numpy as np
import matplotlib.pyplot as plt
from openai import OpenAI
from datetime import datetime
from typing import Dict, List, Optional
from dataclasses import asdict

from .data_models import LLMPriorSpecification, ConsistencyAnalysis

logger = logging.getLogger(__name__)


class ProductionLLMPriorElicitor:
    """
    æœ¬æ ¼çš„ãªLLMäº‹å‰åˆ†å¸ƒè¨­å®šã‚·ã‚¹ãƒ†ãƒ 
    OpenAI GPT-4.1 API ã‚’ä½¿ç”¨
    """
    
    def __init__(self, api_key: str, model: str = "gpt-4.1", temperature: float = 0.2):
        """
        åˆæœŸåŒ–
        
        Parameters:
        -----------
        api_key : str
            OpenAI API ã‚­ãƒ¼ (å¿…é ˆ)
        model : str
            ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ï¼ˆgpt-4.1æ¨å¥¨ï¼‰
        temperature : float
            å¿œç­”ã®ä¸€è²«æ€§åˆ¶å¾¡ï¼ˆ0.1-0.3æ¨å¥¨ï¼‰
        """
        if not api_key:
            raise ValueError("OpenAI API key is required for production use")
        
        self.client = OpenAI(api_key=api_key)
        self.model = model
        self.temperature = temperature
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # APIæ¥ç¶šãƒ†ã‚¹ãƒˆ
        self._test_api_connection()
        
        logger.info(f"âœ“ LLM Prior Elicitor initialized successfully")
        logger.info(f"  Model: {self.model}")
        logger.info(f"  Temperature: {self.temperature}")
        logger.info(f"  Session ID: {self.session_id}")
    
    def _test_api_connection(self):
        """APIæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": "Test connection"}],
                max_tokens=10,
                temperature=0
            )
            logger.info("âœ“ OpenAI API connection successful")
        except Exception as e:
            logger.error(f"âŒ OpenAI API connection failed: {e}")
            raise ConnectionError(f"Cannot connect to OpenAI API: {e}")
    
    def elicit_clinical_priors(self, 
                              treatment_1: str, 
                              treatment_2: str, 
                              outcome_measure: str,
                              clinical_context: str,
                              additional_context: Optional[str] = None) -> List[LLMPriorSpecification]:
        """
        LLMã‹ã‚‰è‡¨åºŠçš„äº‹å‰åˆ†å¸ƒã‚’è¨­å®š
        """
        logger.info("ğŸ”¬ Starting LLM-based prior elicitation...")
        logger.info(f"  Context: {clinical_context}")
        logger.info(f"  Comparison: {treatment_2} vs {treatment_1}")
        logger.info(f"  Outcome: {outcome_measure}")
        
        prompt = self._create_expert_prompt(
            treatment_1, treatment_2, outcome_measure, clinical_context, additional_context
        )
        
        priors = self._get_llm_response(prompt)
        
        logger.info(f"âœ“ Successfully elicited {len(priors)} prior distributions")
        return priors
    
    def _create_expert_prompt(self, treatment_1: str, treatment_2: str, 
                             outcome_measure: str, clinical_context: str,
                             additional_context: Optional[str] = None) -> str:
        """
        å°‚é–€å®¶ç›¸è«‡ç”¨ã®è©³ç´°ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        """
        additional_info = f"\n\nADDITIONAL CLINICAL INFORMATION:\n{additional_context}" if additional_context else ""
        
        prompt = f"""
You are a senior clinical biostatistician and medical expert with extensive experience in {clinical_context}. 

CLINICAL STUDY DESIGN:
- Control Treatment: {treatment_1}
- Experimental Treatment: {treatment_2}
- Primary Outcome: {outcome_measure}
- Clinical Context: {clinical_context}
{additional_info}

STATISTICAL MODEL: {outcome_measure} = Î²â‚€ + Î²â‚Ã—time + Î²â‚‚Ã—(timeÃ—treatment) + Îµ

Please provide expert prior beliefs for each parameter in strict JSON format:
{{
    "expert_assessment": {{
        "baseline_intercept": {{"mean": <value>, "std": <value>, "confidence": <0-1>, "rationale": "<reasoning>"}},
        "time_effect": {{"mean": <value>, "std": <value>, "confidence": <0-1>, "rationale": "<reasoning>"}},
        "treatment_advantage": {{"mean": <value>, "std": <value>, "confidence": <0-1>, "rationale": "<reasoning>"}},
        "error_std": {{"mean": <value>, "std": <value>, "confidence": <0-1>, "rationale": "<reasoning>"}}
    }},
    "overall_confidence": <0-1>
}}
"""
        return prompt
    
    def _get_llm_response(self, prompt: str, max_retries: int = 3) -> List[LLMPriorSpecification]:
        """
        å®Ÿéš›ã®LLM APIå‘¼ã³å‡ºã—ï¼ˆã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ä»˜ãï¼‰
        """
        for attempt in range(max_retries):
            try:
                logger.info(f"ğŸ¤– Consulting LLM expert (attempt {attempt + 1}/{max_retries})...")
                
                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": "You are a clinical biostatistician. Provide precise priors in JSON format."},
                        {"role": "user", "content": prompt}
                    ],
                    temperature=self.temperature,
                    max_tokens=2000
                )
                
                content = response.choices[0].message.content.strip()
                
                # JSONæŠ½å‡º
                if "```json" in content:
                    json_start = content.find("```json") + 7
                    json_end = content.find("```", json_start)
                    content = content[json_start:json_end].strip()
                elif "```" in content:
                    json_start = content.find("```") + 3
                    json_end = content.find("```", json_start)
                    content = content[json_start:json_end].strip()
                
                llm_data = json.loads(content)
                
                # ãƒ‡ãƒ¼ã‚¿å¤‰æ›
                priors = []
                expert_assessment = llm_data.get("expert_assessment", {})
                
                for param_name, param_data in expert_assessment.items():
                    prior = LLMPriorSpecification(
                        parameter=param_name,
                        distribution="normal",
                        mean=float(param_data["mean"]),
                        std=float(param_data["std"]),
                        confidence=float(param_data["confidence"]),
                        rationale=param_data["rationale"],
                        llm_model=self.model,
                        timestamp=datetime.now().isoformat(),
                        session_id=self.session_id
                    )
                    priors.append(prior)
                
                logger.info(f"âœ“ LLM consultation successful")
                return priors
                
            except json.JSONDecodeError as e:
                logger.warning(f"âš ï¸  JSON parsing error (attempt {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    raise ValueError(f"LLM response could not be parsed as JSON: {e}")
                time.sleep(2)
                
            except Exception as e:
                logger.warning(f"âš ï¸  API call failed (attempt {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    raise ConnectionError(f"LLM API call failed: {e}")
                time.sleep(5)
        
        raise RuntimeError("Failed to get valid LLM response after all retries")
    
    def export_priors_for_analysis(self, priors: List[LLMPriorSpecification]) -> Dict:
        """
        ãƒ™ã‚¤ã‚ºè§£æç”¨ã®äº‹å‰åˆ†å¸ƒä»•æ§˜ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
        """
        exported_priors = {}
        
        parameter_mapping = {
            'baseline_intercept': 'beta_intercept',
            'time_effect': 'beta_time',
            'treatment_advantage': 'beta_interaction',
            'error_std': 'sigma'
        }
        
        for prior in priors:
            if prior.parameter in parameter_mapping:
                mapped_name = parameter_mapping[prior.parameter]
                if mapped_name == 'sigma':
                    exported_priors[mapped_name] = {
                        'dist': 'inverse_gamma',
                        'alpha': 1.0,
                        'beta': prior.mean,
                        'source': 'llm_expert',
                        'confidence': prior.confidence,
                        'rationale': prior.rationale
                    }
                else:
                    exported_priors[mapped_name] = {
                        'dist': 'normal',
                        'mu': prior.mean,
                        'sigma': prior.std,
                        'source': 'llm_expert',
                        'confidence': prior.confidence,
                        'rationale': prior.rationale
                    }
        
        logger.info("âœ“ Priors exported for Bayesian analysis")
        return exported_priors
    
    def save_session_data(self, priors: List[LLMPriorSpecification], filename: Optional[str] = None) -> str:
        """
        ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜
        """
        if filename is None:
            filename = f"llm_priors_session_{self.session_id}.json"
        
        session_data = {
            'session_id': self.session_id,
            'model': self.model,
            'temperature': self.temperature,
            'timestamp': datetime.now().isoformat(),
            'priors': [asdict(prior) for prior in priors]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(session_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ“ Session data saved to {filename}")
        return filename


class MockLLMPriorElicitor:
    """
    ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯LLMã‚¨ãƒªã‚·ã‚¿ãƒ¼
    OpenAI APIã‚­ãƒ¼ãŒåˆ©ç”¨ã§ããªã„å ´åˆã®ãƒ†ã‚¹ãƒˆç”¨
    """
    
    def __init__(self, model: str = "mock-gpt-4.1", temperature: float = 0.2):
        """
        åˆæœŸåŒ–
        """
        self.model = model
        self.temperature = temperature
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        logger.info(f"âœ“ Mock LLM Prior Elicitor initialized")
        logger.info(f"  Model: {self.model} (Mock)")
        logger.info(f"  Session ID: {self.session_id}")
    
    def elicit_clinical_priors(self, 
                              treatment_1: str, 
                              treatment_2: str, 
                              outcome_measure: str,
                              clinical_context: str,
                              additional_context: Optional[str] = None) -> List[LLMPriorSpecification]:
        """
        ãƒ¢ãƒƒã‚¯ç‰ˆã®LLMäº‹å‰åˆ†å¸ƒè¨­å®š
        Study2æ­´å²çš„ç ”ç©¶ãƒ‡ãƒ¼ã‚¿ã¨å®Ÿéš›ã®toenailãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆã«åŸºã¥ãè‡¨åºŠçš„åˆç†çš„ãªäº‹å‰åˆ†å¸ƒ
        
        ãƒ‡ãƒ¼ã‚¿æ ¹æ‹ :
        - å®Ÿæ¸¬ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å¹³å‡: 1.89mm (Study2ã§ã¯2.5mmæƒ³å®š)
        - å®Ÿæ¸¬æœˆé–“æˆé•·ç‡: 0.555mm/æœˆ (Itraconazole: 0.558, Terbinafine: 0.600)
        - æ²»ç™‚å„ªä½æ€§: å¹³å‡0.35mm/æœˆ (12ãƒ¶æœˆã§0.771mmå·®)
        - æ¸¬å®šèª¤å·®: å…¨ä½“æ¨™æº–åå·®4.39mm
        """
        logger.info("ğŸ¤– Using Mock LLM for prior elicitation...")
        logger.info(f"  Context: {clinical_context}")
        logger.info(f"  Comparison: {treatment_2} vs {treatment_1}")
        logger.info(f"  Outcome: {outcome_measure}")
        
        # Study2æ­´å²çš„ãƒ‡ãƒ¼ã‚¿ + å®Ÿæ¸¬ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãè‡¨åºŠçš„ã«åˆç†çš„ãªäº‹å‰åˆ†å¸ƒ
        mock_priors = [
            LLMPriorSpecification(
                parameter="baseline_intercept",
                distribution="normal",
                mean=2.5,  # Study2æ­´å²çš„æƒ³å®šå€¤ï¼ˆå®Ÿæ¸¬1.89mmã‚ˆã‚Šä¿å®ˆçš„ï¼‰
                std=1.0,   # é©åº¦ãªä¸ç¢ºå®Ÿæ€§ã‚’è¡¨ç¾
                confidence=0.80,
                rationale="Historical Study2 baseline assumption (2.5mm) vs actual data mean (1.89mm). Conservative prior allows data to inform.",
                llm_model=self.model,
                timestamp=datetime.now().isoformat(),
                session_id=self.session_id
            ),
            LLMPriorSpecification(
                parameter="time_effect",
                distribution="normal", 
                mean=0.6,   # Study2æƒ³å®šå€¤ã€å®Ÿæ¸¬0.558mmã«è¿‘ã„
                std=0.2,    # åˆç†çš„ãªä¸ç¢ºå®Ÿæ€§
                confidence=0.85,
                rationale="Historical Study2 assumption (0.6mm/month) aligns with actual Itraconazole rate (0.558mm/month).",
                llm_model=self.model,
                timestamp=datetime.now().isoformat(),
                session_id=self.session_id
            ),
            LLMPriorSpecification(
                parameter="treatment_advantage",
                distribution="normal",
                mean=0.0,   # Study2ä¸­æ€§æƒ³å®šã€å®Ÿæ¸¬ã§ã¯0.042mm/æœˆå·®
                std=0.15,   # å°ã•ã„ãŒæ¤œå‡ºå¯èƒ½ãªåŠ¹æœã‚’è¨±å®¹
                confidence=0.70,
                rationale="Historical Study2 neutral assumption (0.0). Actual data shows 0.042mm/month advantage for Terbinafine.",
                llm_model=self.model,
                timestamp=datetime.now().isoformat(),
                session_id=self.session_id
            ),
            LLMPriorSpecification(
                parameter="error_std",
                distribution="normal",
                mean=3.7,   # Study2æ­´å²çš„æƒ³å®šï¼ˆå®Ÿæ¸¬4.39mmã‚ˆã‚Šä¿å®ˆçš„ï¼‰
                std=0.8,    # æ¸¬å®šèª¤å·®ã®ä¸ç¢ºå®Ÿæ€§
                confidence=0.85,
                rationale="Historical Study2 error assumption (3.7mm) vs actual data std (4.39mm). Prior informed by clinical experience.",
                llm_model=self.model,
                timestamp=datetime.now().isoformat(),
                session_id=self.session_id
            )
        ]
        
        logger.info(f"âœ“ Mock LLM generated {len(mock_priors)} prior distributions")
        return mock_priors
    
    def export_priors_for_analysis(self, priors: List[LLMPriorSpecification]) -> Dict:
        """
        ãƒ™ã‚¤ã‚ºè§£æç”¨ã®äº‹å‰åˆ†å¸ƒä»•æ§˜ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆï¼ˆMockç‰ˆï¼‰
        """
        exported_priors = {}
        
        parameter_mapping = {
            'baseline_intercept': 'beta_intercept',
            'time_effect': 'beta_time',
            'treatment_advantage': 'beta_interaction',
            'error_std': 'sigma'
        }
        
        for prior in priors:
            if prior.parameter in parameter_mapping:
                mapped_name = parameter_mapping[prior.parameter]
                if mapped_name == 'sigma':
                    exported_priors[mapped_name] = {
                        'dist': 'inverse_gamma',
                        'alpha': 1.0,
                        'beta': prior.mean,
                        'source': 'mock_llm_expert',
                        'confidence': prior.confidence,
                        'rationale': prior.rationale
                    }
                else:
                    exported_priors[mapped_name] = {
                        'dist': 'normal',
                        'mu': prior.mean,
                        'sigma': prior.std,
                        'source': 'mock_llm_expert',
                        'confidence': prior.confidence,
                        'rationale': prior.rationale
                    }
        
        logger.info("âœ“ Mock priors exported for Bayesian analysis")
        return exported_priors
    
    def save_session_data(self, priors: List[LLMPriorSpecification], filename: Optional[str] = None) -> str:
        """
        ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜ï¼ˆMockç‰ˆï¼‰
        """
        if filename is None:
            filename = f"mock_llm_priors_session_{self.session_id}.json"
        
        session_data = {
            'session_id': self.session_id,
            'model': self.model,
            'temperature': self.temperature,
            'timestamp': datetime.now().isoformat(),
            'priors': [asdict(prior) for prior in priors]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(session_data, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ“ Mock session data saved to {filename}")
        return filename
