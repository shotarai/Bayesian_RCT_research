# filepath: src/analysis.py
"""
Analysis Functions for Bayesian RCT Research
ãƒ™ã‚¤ã‚ºRCTç ”ç©¶ç”¨ã®åˆ†ææ©Ÿèƒ½
"""

import numpy as np
import logging
from typing import Dict, List, Optional

from .data_models import PriorComparison, SampleSizeComparison
from .llm_elicitor import ProductionLLMPriorElicitor, MockLLMPriorElicitor
from .data_loader import load_historical_priors

logger = logging.getLogger(__name__)


def comparative_analysis_setup(api_key: Optional[str] = None):
    """
    LLMäº‹å‰åˆ†å¸ƒã¨ä»¥å‰ã®ç ”ç©¶ã®äº‹å‰åˆ†å¸ƒã‚’æ¯”è¼ƒã™ã‚‹ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
    APIã‚­ãƒ¼ãŒãªã„å ´åˆã¯MockLLMPriorElicitorã‚’ä½¿ç”¨
    """
    print("\n" + "="*80)
    print("COMPARATIVE ANALYSIS: LLM vs HISTORICAL PRIORS")
    print("="*80)
    
    # LLMäº‹å‰åˆ†å¸ƒã®å–å¾—ï¼ˆAPIã‚­ãƒ¼ã®æœ‰ç„¡ã§åˆ‡ã‚Šæ›¿ãˆï¼‰
    if api_key:
        try:
            elicitor = ProductionLLMPriorElicitor(api_key=api_key)
        except Exception as e:
            logger.warning(f"âš ï¸ Production LLM failed: {e}")
            logger.info("ğŸ”„ Falling back to Mock LLM")
            elicitor = MockLLMPriorElicitor()
    else:
        logger.info("ğŸ¤– Using Mock LLM (no API key provided)")
        elicitor = MockLLMPriorElicitor()
    
    llm_priors = elicitor.elicit_clinical_priors(
        treatment_1="Itraconazole 250mg daily for 12 weeks",
        treatment_2="Terbinafine 250mg daily for 12 weeks",
        outcome_measure="unaffected nail length in millimeters", 
        clinical_context="toenail fungal infection (onychomycosis) in adults"
    )
    
    # ä»¥å‰ã®ç ”ç©¶ã®äº‹å‰åˆ†å¸ƒ
    historical_priors = load_historical_priors()
    
    # LLMäº‹å‰åˆ†å¸ƒã®å¤‰æ›
    llm_analysis_priors = elicitor.export_priors_for_analysis(llm_priors)
    
    comparison_setup = {
        'historical_fixed': historical_priors['fixed_effect_model'],
        'historical_mixed': historical_priors['mixed_effect_model'], 
        'llm_expert': llm_analysis_priors,
        'uninformative': {
            'beta_intercept': {'dist': 'normal', 'mu': 0, 'sigma': 100},
            'beta_time': {'dist': 'normal', 'mu': 0, 'sigma': 100}, 
            'beta_interaction': {'dist': 'normal', 'mu': 0, 'sigma': 100},
            'sigma': {'dist': 'half_normal', 'sigma': 100}
        }
    }
    
    return comparison_setup, llm_priors


def compare_prior_specifications(llm_priors: Dict, historical_priors: Dict) -> List[PriorComparison]:
    """
    LLMäº‹å‰åˆ†å¸ƒã¨æ­´å²çš„äº‹å‰åˆ†å¸ƒã®è©³ç´°æ¯”è¼ƒ
    """
    comparisons = []
    
    for param in ['beta_intercept', 'beta_time', 'beta_interaction']:
        if param in llm_priors and param in historical_priors:
            llm = llm_priors[param]
            hist = historical_priors[param]
            
            # å¹³å‡ã¨æ¨™æº–åå·®ã®å·®ç•°
            diff_mean = llm['mu'] - hist['mu']
            diff_std = llm['sigma'] - hist['sigma']
            
            # é‡è¤‡ä¿‚æ•°ã®è¨ˆç®—
            overlap = calculate_distribution_overlap(llm['mu'], llm['sigma'], hist['mu'], hist['sigma'])
            
            comparison = PriorComparison(
                parameter=param,
                llm_prior=llm,
                historical_prior=hist,
                difference_mean=diff_mean,
                difference_std=diff_std,
                overlap_coefficient=overlap
            )
            comparisons.append(comparison)
    
    return comparisons


def calculate_distribution_overlap(mu1: float, sigma1: float, mu2: float, sigma2: float) -> float:
    """
    2ã¤ã®æ­£è¦åˆ†å¸ƒã®é‡è¤‡ä¿‚æ•°ã‚’è¨ˆç®—
    """
    combined_variance = (sigma1**2 + sigma2**2) / 2
    distance = abs(mu1 - mu2)
    overlap = np.exp(-0.25 * distance**2 / combined_variance)
    return overlap


def calculate_sample_size_benefits(comparison_setup: Dict) -> List[SampleSizeComparison]:
    """
    äº‹å‰åˆ†å¸ƒã«ã‚ˆã‚‹æƒ…å ±é‡ã®æ¯”è¼ƒã¨ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºå‰Šæ¸›åŠ¹æœã®è¨ˆç®—
    """
    results = []
    
    # å„äº‹å‰åˆ†å¸ƒã®æƒ…å ±é‡ã‚’è¨ˆç®—
    for prior_name, priors in comparison_setup.items():
        if prior_name == 'uninformative':
            continue
            
        # äº‹å‰åˆ†å¸ƒã®ç²¾åº¦ï¼ˆåˆ†æ•£ã®é€†æ•°ï¼‰ã‚’è¨ˆç®—
        beta_precision = sum([
            1/priors[param]['sigma']**2 
            for param in ['beta_intercept', 'beta_time', 'beta_interaction'] 
            if param in priors
        ])
        
        # ç„¡æƒ…å ±äº‹å‰åˆ†å¸ƒã¨ã®æ¯”è¼ƒ
        uninformative_precision = sum([
            1/comparison_setup['uninformative'][param]['sigma']**2 
            for param in ['beta_intercept', 'beta_time', 'beta_interaction']
        ])
        
        # æƒ…å ±åˆ©å¾—ã®è¨ˆç®—
        information_gain = beta_precision / uninformative_precision if uninformative_precision > 0 else 1
        
        # ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºå‰Šæ¸›åŠ¹æœã®æ¨å®š
        sample_size_reduction = (information_gain - 1) / information_gain if information_gain > 1 else 0
        
        # ä»®æƒ³çš„ãªåŸºæº–ã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚ºã‹ã‚‰ã®å‰Šæ¸›
        baseline_n = 400  # çˆªçœŸèŒç—‡ç ”ç©¶ã®å…¸å‹çš„ãªã‚µãƒ³ãƒ—ãƒ«ã‚µã‚¤ã‚º
        patients_saved = int(baseline_n * sample_size_reduction)
        
        results.append(SampleSizeComparison(
            prior_type=prior_name,
            effective_sample_size=information_gain,
            power=0.8 + 0.1 * information_gain,
            sample_size_reduction=sample_size_reduction,
            patient_savings=patients_saved
        ))
    
    return results
